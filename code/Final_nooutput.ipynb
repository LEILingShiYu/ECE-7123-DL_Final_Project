{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Env setting"
      ],
      "metadata": {
        "id": "b6Dy1qwXQgeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To use nyu depth v2 dataset in hugging face, use datasets==3.6.0\n",
        "!pip install datasets==3.6\n",
        "!pip install ptflops"
      ],
      "metadata": {
        "id": "rJU287eDPz3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import"
      ],
      "metadata": {
        "id": "wQzaKi8RDZXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# save path\n",
        "save_dir = '/content/drive/MyDrive/DepthProject'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Model save path: {save_dir}\")"
      ],
      "metadata": {
        "id": "3WtHQWggF_BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "import timm\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n"
      ],
      "metadata": {
        "id": "r-ppwy-lDkmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config"
      ],
      "metadata": {
        "id": "OK_xOAWCDnLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    \"backbone\": \"swin_tiny_patch4_window7_224\",\n",
        "    \"batch_size\": 128,\n",
        "    \"lr\": 1e-3,\n",
        "    \"epochs\": 40,\n",
        "    \"image_size\": (224, 224),\n",
        "    \"max_depth\": 10.0,      # Dataset standard\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "}\n",
        "\n",
        "print(f\"device: {CONFIG['device']}\")\n"
      ],
      "metadata": {
        "id": "JYq81W5MDvB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "Mz3FvE0DDxmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HF_NYUDepthDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, transform=None, is_train=True):\n",
        "        self.dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "        self.is_train = is_train\n",
        "        self.config = CONFIG\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "\n",
        "        # Image/RGB, Depth/Depth_map\n",
        "        if 'image' in sample:\n",
        "            image = sample['image']\n",
        "        elif 'rgb' in sample:\n",
        "            image = sample['rgb']\n",
        "        else:\n",
        "            raise KeyError(f\"Image not found. Keys: {sample.keys()}\")\n",
        "\n",
        "        if 'depth_map' in sample:\n",
        "            depth = sample['depth_map'] # For 'Raw' Dataset in NYU-V2\n",
        "        elif 'depth' in sample:\n",
        "            depth = sample['depth']     # For 'Standard'\n",
        "        else:\n",
        "            raise KeyError(f\"Depth not found. Keys: {sample.keys()}\")\n",
        "\n",
        "        # Turn List/Numpy to Tensor\n",
        "\n",
        "        if isinstance(depth, list):\n",
        "            depth = np.array(depth, dtype=np.float32)\n",
        "\n",
        "        if isinstance(depth, np.ndarray):\n",
        "            depth = torch.from_numpy(depth).float()\n",
        "            if depth.ndim == 2:\n",
        "                depth = depth.unsqueeze(0)\n",
        "\n",
        "        # Resize\n",
        "        image = TF.resize(image, self.config['image_size'], interpolation=TF.InterpolationMode.BILINEAR)\n",
        "        depth = TF.resize(depth, self.config['image_size'], interpolation=TF.InterpolationMode.NEAREST)\n",
        "\n",
        "        image = TF.to_tensor(image) # PIL 0-255 to 0-1 Tensor\n",
        "        image = TF.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        if not isinstance(depth, torch.Tensor):\n",
        "            depth = TF.to_tensor(depth)\n",
        "\n",
        "        # Scaling\n",
        "        current_max = depth.max().item()\n",
        "\n",
        "        if current_max > 100.0:\n",
        "            # mm\n",
        "            depth = depth / 1000.0\n",
        "        elif current_max > 20.0:\n",
        "\n",
        "            depth = depth / 10.0\n",
        "        elif current_max < 1.1:\n",
        "            # 0-1\n",
        "            depth = depth * 10.0\n",
        "        # clamp\n",
        "        depth = torch.clamp(depth, 0, self.config['max_depth'])\n",
        "\n",
        "        return image, depth\n",
        "print(\"Downloading HuggingFace Dataset...\")\n",
        "Train_data = load_dataset(\"sayakpaul/nyu_depth_v2\", split=\"train\",trust_remote_code=True,verification_mode=\"no_checks\",num_proc=8)\n",
        "\n",
        "#small_data = Train_data.select(range(10000))\n",
        "\n",
        "Val_data = load_dataset(\"sayakpaul/nyu_depth_v2\", split=\"validation\",trust_remote_code=True,verification_mode=\"no_checks\",num_proc=8)\n",
        "\n",
        "\n",
        "train_dataset = HF_NYUDepthDataset(Train_data, is_train=True)\n",
        "val_dataset = HF_NYUDepthDataset(Val_data, is_train=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=6,pin_memory=True,persistent_workers=True,drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=6,pin_memory=True,persistent_workers=True)\n",
        "\n",
        "print(f\"Train set: {len(train_dataset)} images, Val set: {len(val_dataset)} images\")"
      ],
      "metadata": {
        "id": "JrTkF2fcDz8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "5XE_dOrqD4-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchFusionBlock(nn.Module):\n",
        "    def __init__(self, out_channels):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        if x.shape[-2:] != skip.shape[-2:]:\n",
        "            x = F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=True)\n",
        "        return self.layer(x + skip)\n",
        "\n",
        "class LightweightDPT(nn.Module):\n",
        "    def __init__(self, backbone_name, max_depth=10.0):\n",
        "        super().__init__()\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "        # 1. Backbone (Swin / ResNet)\n",
        "        self.backbone = timm.create_model(backbone_name, pretrained=True, features_only=True)\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.randn(1, 3, 224, 224)\n",
        "            feats = self.backbone(dummy)\n",
        "            ch = [f.shape[1] for f in feats]\n",
        "\n",
        "        self.feat_channels = ch[-4:]\n",
        "\n",
        "        # 2. Decoder Components\n",
        "        embed_dim = 128\n",
        "\n",
        "        self.projs = nn.ModuleList([\n",
        "            nn.Conv2d(c, embed_dim, 1) for c in self.feat_channels\n",
        "        ])\n",
        "\n",
        "        self.fusion_blocks = nn.ModuleList([\n",
        "            ScratchFusionBlock(embed_dim),\n",
        "            ScratchFusionBlock(embed_dim),\n",
        "            ScratchFusionBlock(embed_dim),\n",
        "        ])\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim, 64, 3, 1, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
        "            nn.Conv2d(64, 32, 3, 1, 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
        "            nn.Conv2d(32, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)[-4:]\n",
        "        projs = [p(f) for p, f in zip(self.projs, features)]\n",
        "\n",
        "        out = projs[3]\n",
        "        out = self.fusion_blocks[0](out, projs[2])\n",
        "        out = self.fusion_blocks[1](out, projs[1])\n",
        "        out = self.fusion_blocks[2](out, projs[0])\n",
        "\n",
        "        depth = self.head(out)\n",
        "\n",
        "        if depth.shape[-2:] != x.shape[-2:]:\n",
        "            depth = F.interpolate(depth, size=x.shape[-2:], mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "        # Sigmoid output 0~1, then multiple max_depth to predicted depth 0~10m\n",
        "        return depth * self.max_depth"
      ],
      "metadata": {
        "id": "TsjJRpK8ECUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss"
      ],
      "metadata": {
        "id": "JoUcp3DnELBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SILogLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"SILog\"\n",
        "\n",
        "    def forward(self, input, target, mask=None):\n",
        "        if mask is not None:\n",
        "            input = input[mask]\n",
        "            target = target[mask]\n",
        "\n",
        "        if input.numel() < 2:\n",
        "\n",
        "            if input.numel() > 0:\n",
        "\n",
        "                return torch.mean(input) * 0.0\n",
        "            else:\n",
        "                return torch.tensor(0.0, device=input.device, requires_grad=True)\n",
        "\n",
        "        min_depth_clamp = 1e-3\n",
        "        log_input = torch.log(torch.clamp(input, min=min_depth_clamp))\n",
        "        log_target = torch.log(torch.clamp(target, min=min_depth_clamp))\n",
        "\n",
        "        g = log_input - log_target\n",
        "\n",
        "        dg = torch.var(g) + 0.15 * torch.pow(torch.mean(g), 2)\n",
        "        return 10 * torch.sqrt(dg)"
      ],
      "metadata": {
        "id": "Djc52NhEEHHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics"
      ],
      "metadata": {
        "id": "YfteZjMqdK7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_errors(gt, pred):\n",
        "    \"\"\"\n",
        "    AbsRel, RMSE, Log10, Delta1, Delta2, Delta3\n",
        "    \"\"\"\n",
        "    thresh = torch.max((gt / pred), (pred / gt))\n",
        "    a1 = (thresh < 1.25).float().mean()\n",
        "    a2 = (thresh < 1.25 ** 2).float().mean()\n",
        "    a3 = (thresh < 1.25 ** 3).float().mean()\n",
        "\n",
        "    rmse = (gt - pred) ** 2\n",
        "    rmse = torch.sqrt(rmse.mean())\n",
        "\n",
        "    rmse_log = (torch.log(gt) - torch.log(pred)) ** 2\n",
        "    rmse_log = torch.sqrt(rmse_log.mean())\n",
        "\n",
        "    abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n",
        "\n",
        "    sq_rel = torch.mean(((gt - pred) ** 2) / gt)\n",
        "\n",
        "    return abs_rel, rmse, a1, a2, a3\n",
        "\n",
        "def validate_model(model, loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    depth_loss = 0.0\n",
        "\n",
        "    metrics = {'abs_rel': 0, 'rmse': 0, 'a1': 0, 'a2': 0, 'a3': 0}\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, depths in loader:\n",
        "            images = images.to(device)\n",
        "            depths = depths.to(device)\n",
        "\n",
        "            preds = model(images)\n",
        "            preds = torch.clamp(preds, min=1e-3, max=10.0)\n",
        "\n",
        "            mask = (depths > 0.1) & (depths < 10.0)\n",
        "\n",
        "            if mask.sum() > 0:\n",
        "\n",
        "                valid_gt = depths[mask]\n",
        "                valid_pred = preds[mask]\n",
        "\n",
        "                abs_rel, rmse, a1, a2, a3 = compute_errors(valid_gt, valid_pred)\n",
        "\n",
        "                metrics['abs_rel'] += abs_rel.item()\n",
        "                metrics['rmse'] += rmse.item()\n",
        "                metrics['a1'] += a1.item()\n",
        "                metrics['a2'] += a2.item()\n",
        "                metrics['a3'] += a3.item()\n",
        "                count += 1\n",
        "\n",
        "    for k in metrics.keys():\n",
        "        metrics[k] /= count\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "WXxv3p73dJwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comaprision"
      ],
      "metadata": {
        "id": "gCw5Nl9JdWXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Swin-Tiny\n",
        "swin_model = LightweightDPT(backbone_name=\"swin_tiny_patch4_window7_224\", max_depth=CONFIG['max_depth'])\n",
        "\n",
        "# ResNet-34\n",
        "resnet_model = LightweightDPT(backbone_name=\"resnet34\", max_depth=CONFIG['max_depth'])"
      ],
      "metadata": {
        "id": "OgnAwFwqdVfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from ptflops import get_model_complexity_info\n",
        "with torch.no_grad():\n",
        "    swin_macs, swin_params = get_model_complexity_info(\n",
        "        swin_model,\n",
        "        (3, 224, 224),\n",
        "        as_strings=True,\n",
        "        print_per_layer_stat=False,\n",
        "    )\n",
        "with torch.no_grad():\n",
        "    Res_macs, Res_params = get_model_complexity_info(\n",
        "        resnet_model,\n",
        "        (3, 224, 224),\n",
        "        as_strings=True,\n",
        "        print_per_layer_stat=False,\n",
        "    )\n",
        "print(\"Swin_MACs:\", swin_macs)\n",
        "print(\"Swin_Params:\", swin_params)\n",
        "print(\"Res_MACs:\", Res_macs)\n",
        "print(\"Res_Params:\", Res_params)"
      ],
      "metadata": {
        "id": "h9SFruuCoPXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swin_model = torch.compile(swin_model)\n",
        "resnet_model = torch.compile(resnet_model)"
      ],
      "metadata": {
        "id": "K2QZSKfOqJ58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "VA4s8JxfEI0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_batch(model, loader, device):\n",
        "    model.eval()\n",
        "    images, depths = next(iter(loader))\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = model(images)\n",
        "\n",
        "    img = images[1].cpu().permute(1, 2, 0).numpy()\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    depth_gt = depths[1, 0].cpu().numpy()\n",
        "    depth_pred = preds[1, 0].cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axs[0].imshow(img)\n",
        "    axs[0].set_title(\"Input RGB\")\n",
        "\n",
        "    axs[1].imshow(depth_gt, cmap='inferno', vmin=0, vmax=10)\n",
        "    axs[1].set_title(\"Ground Truth Depth\")\n",
        "\n",
        "    im = axs[2].imshow(depth_pred, cmap='inferno', vmin=0, vmax=10)\n",
        "    axs[2].set_title(\"Predicted Depth\")\n",
        "\n",
        "    plt.colorbar(im, ax=axs[2])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nV_CsQo3EQy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(model_name, train_losses, val_losses):\n",
        "    \"\"\"\n",
        "    Plots the training loss (SILogLoss) and validation loss (RMSE) over epochs.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot Training Loss\n",
        "    plt.plot(epochs, train_losses, 'o-', color='blue', label=f'{model_name} Training Loss (SILog)')\n",
        "\n",
        "    # Plot Validation Loss\n",
        "    plt.plot(epochs, val_losses, 'o-', color='red', label=f'{model_name} Validation Loss (RMSE)')\n",
        "\n",
        "    plt.title(f'Loss Curves for {model_name} Model')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2DevRqu7-dDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_and_evaluate(model_name, model, train_loader, val_loader, save_dir, epochs=3):\n",
        "    device = CONFIG['device']\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=0.1)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
        "    criterion = SILogLoss()\n",
        "\n",
        "    # Initialize loss and metric trackers\n",
        "    train_losses_history = []\n",
        "    val_losses_history = []\n",
        "\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "    save_path = os.path.join(save_dir, f\"{model_name}_best.pth\")\n",
        "    best_a1 = 0.0\n",
        "\n",
        "    print(f\"\\n {model_name} AMP + Cosine Annealing ...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        for param in model.parameters():\n",
        "            if not param.requires_grad:\n",
        "                param.requires_grad = True\n",
        "\n",
        "        train_loss = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for imgs, gts in pbar:\n",
        "            imgs, gts = imgs.to(device), gts.to(device)\n",
        "            mask = (gts > 0.1) & (gts < CONFIG['max_depth'])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                preds = model(imgs)\n",
        "                preds = torch.clamp(preds, min=1e-3, max=CONFIG['max_depth'])\n",
        "                loss = criterion(preds, gts, mask)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        # Calculate and record average training loss for the epoch\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses_history.append(avg_train_loss)\n",
        "\n",
        "        scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        val_metrics = validate_model(model, val_loader, device)\n",
        "        current_rmse = val_metrics['rmse']\n",
        "        current_a1 = val_metrics['a1']\n",
        "        val_losses_history.append(current_rmse)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} (LR={current_lr:.2e}): \"\n",
        "              f\"Loss={train_loss/len(train_loader):.3f} | \"\n",
        "              f\"AbsRel={val_metrics['abs_rel']:.3f} | \"\n",
        "              f\"RMSE={val_metrics['rmse']:.3f} | \"\n",
        "              f\"δ1={val_metrics['a1']:.3f}\")\n",
        "\n",
        "        if current_a1 > best_a1:\n",
        "            best_a1 = current_a1\n",
        "\n",
        "            print(f\"Higher acc (δ1: {best_a1:.3f})! Saving...\")\n",
        "\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_a1': best_a1,\n",
        "                'config': CONFIG\n",
        "            }, save_path)\n",
        "\n",
        "            print(f\"Model has been saved: {save_path}\")\n",
        "\n",
        "    return val_metrics, train_losses_history, val_losses_history\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    swin_results, swin_train_loss, swin_val_loss = train_and_evaluate(\n",
        "        \"Swin-Tiny\", swin_model, train_loader, val_loader, save_dir, epochs=CONFIG['epochs']\n",
        "    )\n",
        "\n",
        "    resnet_results, resnet_train_loss, resnet_val_loss = train_and_evaluate(\n",
        "        \"ResNet-34\", resnet_model, train_loader, val_loader, save_dir, epochs=CONFIG['epochs']\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"{'Metric':<10} | {'Swin-Tiny (Ours)':<18} | {'ResNet-34 (Base)':<18}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'AbsRel':<10} | {swin_results['abs_rel']:<18.4f} | {resnet_results['abs_rel']:<18.4f}\")\n",
        "    print(f\"{'RMSE':<10} | {swin_results['rmse']:<18.4f} | {resnet_results['rmse']:<18.4f}\")\n",
        "    print(f\"{'δ1 (Acc)':<10} | {swin_results['a1']:<18.4f} | {resnet_results['a1']:<18.4f}\")\n",
        "    print(f\"{'δ2 ':<10} | {swin_results['a2']:<18.4f} | {resnet_results['a2']:<18.4f}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"Visualize Swin-Tiny:\")\n",
        "    visualize_batch(swin_model, val_loader, CONFIG['device'])"
      ],
      "metadata": {
        "id": "ccWrrhgfDXHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot Loss Curves ---\n",
        "print(\"\\nVisualizing Loss Curves:\")\n",
        "plot_loss_curves(\"Swin-Tiny\", swin_train_loss, swin_val_loss)\n",
        "plot_loss_curves(\"ResNet-34\", resnet_train_loss, resnet_val_loss)"
      ],
      "metadata": {
        "id": "DOtmnuly-k9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*65)\n",
        "print(f\"{'Metric':<12} | {'Swin-Tiny (Ours)':<20} | {'ResNet-34 (Base)':<20}\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Error\n",
        "print(f\"{'AbsRel (↓)':<12} | {swin_results['abs_rel']:<20.4f} | {resnet_results['abs_rel']:<20.4f}\")\n",
        "print(f\"{'RMSE   (↓)':<12} | {swin_results['rmse']:<20.4f} | {resnet_results['rmse']:<20.4f}\")\n",
        "\n",
        "\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# ACC\n",
        "print(f\"{'δ1     (↑)':<12} | {swin_results['a1']:<20.4f} | {resnet_results['a1']:<20.4f}\")\n",
        "print(f\"{'δ2     (↑)':<12} | {swin_results['a2']:<20.4f} | {resnet_results['a2']:<20.4f}\")\n",
        "print(f\"{'δ3     (↑)':<12} | {swin_results['a3']:<20.4f} | {resnet_results['a3']:<20.4f}\")\n",
        "\n",
        "print(\"=\"*65)"
      ],
      "metadata": {
        "id": "RKWNdMdcHqNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetune"
      ],
      "metadata": {
        "id": "5fRVDkohG1_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "Standard_Full = load_dataset(\"0jl/NYUv2\", split=\"train\", trust_remote_code=True)\n",
        "# 795 Training Set\n",
        "Standard_Train = Standard_Full.select(range(795))\n",
        "# 654 Testing Set\n",
        "Standard_Test = Standard_Full.select(range(795, 1449))\n",
        "\n",
        "ft_train_dataset = HF_NYUDepthDataset(Standard_Train, is_train=True)\n",
        "ft_val_dataset = HF_NYUDepthDataset(Standard_Test, is_train=False)\n",
        "\n",
        "ft_train_loader = DataLoader(ft_train_dataset, batch_size=16, shuffle=True, num_workers=0,pin_memory=True)\n",
        "ft_val_loader = DataLoader(ft_val_dataset, batch_size=16, shuffle=False, num_workers=0,pin_memory=True)\n"
      ],
      "metadata": {
        "id": "7Aap_0KMGXxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Fine-tuning Swin-Tiny...\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "load_path = '/content/drive/MyDrive/DepthProject/Swin-Tiny_best.pth'\n",
        "\n",
        "if os.path.exists(load_path):\n",
        "    checkpoint = torch.load(load_path)\n",
        "    swin_ft_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"δ1: {checkpoint['best_a1']:.3f}\")\n",
        "else:\n",
        "    print(\"File doesn't exist\")\n",
        "\n",
        "# Hyperparameter\n",
        "CONFIG['lr'] = 1e-5\n",
        "\n",
        "print(\"Start Fine-tuning on 795 images\")\n",
        "ft_swin_results, ft_swin_loss, ft_swin_val_loss = train_and_evaluate(\"Swin-Tiny-FT\", swin_ft_model, ft_train_loader, ft_val_loader, save_dir,epochs=10)\n"
      ],
      "metadata": {
        "id": "nQRLKaeVFj4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*65)\n",
        "print(f\"{'Metric':<12} | {'Swin-Tiny':<20}\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "print(f\"{'AbsRel (↓)':<12} | {ft_swin_results['abs_rel']:<20.4f}\")\n",
        "print(f\"{'RMSE   (↓)':<12} | {ft_swin_results['rmse']:<20.4f}\")\n",
        "\n",
        "print(\"-\" * 65)\n",
        "\n",
        "print(f\"{'δ1     (↑)':<12} | {ft_swin_results['a1']:<20.4f}\")\n",
        "print(f\"{'δ2     (↑)':<12} | {ft_swin_results['a2']:<20.4f}\")\n",
        "print(f\"{'δ3     (↑)':<12} | {ft_swin_results['a3']:<20.4f}\")"
      ],
      "metadata": {
        "id": "KLzUCmaO0Dap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fine-tuning Resnet...\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "load_path = '/content/drive/MyDrive/DepthProject/ResNet-34_best.pth'\n",
        "\n",
        "if os.path.exists(load_path):\n",
        "    checkpoint = torch.load(load_path)\n",
        "    resnet_ft_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"δ1: {checkpoint['best_a1']:.3f}\")\n",
        "else:\n",
        "    print(\"File doesn't exist\")\n",
        "\n",
        "CONFIG['lr'] = 1e-5\n",
        "\n",
        "print(\"Start Fine-tuning on 795 images\")\n",
        "ft_res_results, ft_res_loss, ft_res_val_loss = train_and_evaluate(\"ResNet-34-FT\", resnet_ft_model, ft_train_loader, ft_val_loader, save_dir,epochs=10)"
      ],
      "metadata": {
        "id": "kQhcL_9K0dNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*65)\n",
        "print(f\"{'Metric':<12} | {'ResNet-34 (Base)':<20}\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "print(f\"{'AbsRel (↓)':<12} | {ft_res_results['abs_rel']:<20.4f}\")\n",
        "print(f\"{'RMSE   (↓)':<12} | {ft_res_results['rmse']:<20.4f}\")\n",
        "\n",
        "print(\"-\" * 65)\n",
        "\n",
        "print(f\"{'δ1     (↑)':<12} | {ft_res_results['a1']:<20.4f}\")\n",
        "print(f\"{'δ2     (↑)':<12} | {ft_res_results['a2']:<20.4f}\")\n",
        "print(f\"{'δ3     (↑)':<12} | {ft_res_results['a3']:<20.4f}\")"
      ],
      "metadata": {
        "id": "tAPL-pBW0s_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot Loss Curves ---\n",
        "print(\"\\nVisualizing Loss Curves:\")\n",
        "plot_loss_curves(\"Swin-Tiny-FT\", ft_swin_loss, ft_swin_val_loss)\n",
        "plot_loss_curves(\"ResNet-34-FT\", ft_res_loss, ft_res_val_loss)\n",
        "visualize_batch(swin_ft_model, ft_val_loader, CONFIG['device'])\n",
        "visualize_batch(resnet_ft_model, ft_val_loader, CONFIG['device'])\n"
      ],
      "metadata": {
        "id": "34NtmKSF_PVN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}